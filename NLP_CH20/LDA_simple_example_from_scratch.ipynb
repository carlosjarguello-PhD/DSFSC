{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA from scratch - Following Grus' code \n",
    "\n",
    "Simple example of topic modeling using LDA and Gibbs sampling. Each document has a \n",
    "districution of topics, and each topic has a distribution of weights. The idea is to \n",
    "use Gibbs sampling to get the join probability distributions of topics/words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to sample index according to input weights (probabilities). Needed for Gibbs sampling.\n",
    "import random\n",
    "def sample_from(weights):\n",
    "    num = sum(weights)*random.random()\n",
    "    c = 0\n",
    "    for i,x in enumerate(weights):\n",
    "        c += x\n",
    "        if num < c:\n",
    "            return i\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 595, 0: 206, 1: 199})\n"
     ]
    }
   ],
   "source": [
    "#Testing sampling function\n",
    "from collections import Counter\n",
    "test = []\n",
    "for _ in range(1000):\n",
    "    test.append(sample_from([1,1,3]))\n",
    "print Counter(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Following Grus' example. Let's try to group these \"documents\" in 4 topics\n",
    "\n",
    "documents = [\n",
    "    [\"Hadoop\", \"Big Data\", \"HBase\", \"Java\", \"Spark\", \"Storm\", \"Cassandra\"],\n",
    "    [\"NoSQL\", \"MongoDB\", \"Cassandra\", \"HBase\", \"Postgres\"],\n",
    "    [\"Python\", \"scikit-learn\", \"scipy\", \"numpy\", \"statsmodels\", \"pandas\"],\n",
    "    [\"R\", \"Python\", \"statistics\", \"regression\", \"probability\"],\n",
    "    [\"machine learning\", \"regression\", \"decision trees\", \"libsvm\"],\n",
    "    [\"Python\", \"R\", \"Java\", \"C++\", \"Haskell\", \"programming languages\"],\n",
    "    [\"statistics\", \"probability\", \"mathematics\", \"theory\"],\n",
    "    [\"machine learning\", \"scikit-learn\", \"Mahout\", \"neural networks\"],\n",
    "    [\"neural networks\", \"deep learning\", \"Big Data\", \"artificial intelligence\"],\n",
    "    [\"Hadoop\", \"Java\", \"MapReduce\", \"Big Data\"],\n",
    "    [\"statistics\", \"R\", \"statsmodels\"],\n",
    "    [\"C++\", \"deep learning\", \"artificial intelligence\", \"probability\"],\n",
    "    [\"pandas\", \"R\", \"Python\"],\n",
    "    [\"databases\", \"HBase\", \"Postgres\", \"MySQL\", \"MongoDB\"],\n",
    "    [\"libsvm\", \"regression\", \"support vector machines\"]\n",
    "]\n",
    "\n",
    "#Number of topics:\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now some definitions for the problem. \n",
    "\n",
    "- Each document has a certain distribution of topics. This distribution depends on the words that each document has. So, we need to count the number of times that a certain topic appears in a document.\n",
    "- Further, a given word may belong to different topics, appearing more or less frequently. Then, the probability of a word belonging to a certain topic is calculated as the ratio of the times that word appear in a topic divided by the total number of words per topic?\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Definitions below:\n",
    "\n",
    "#How many times each topic is assigned to each document?\n",
    "document_topic_counts = [Counter() for _ in documents]\n",
    "\n",
    "#How many times each word is assigned to each topic?\n",
    "topic_word_counts = [Counter() for _ in range(K)]\n",
    "\n",
    "# Total number of words contained per topic:\n",
    "topic_counts = [0 for _ in range(K)]\n",
    "\n",
    "#Total number of words per document:\n",
    "document_lengths = map(len, documents)\n",
    "\n",
    "# Number of distict words:\n",
    "distinct_words = set(word for document in documents for word in document)\n",
    "W = len(distinct_words)\n",
    "\n",
    "#Number of documents\n",
    "D = len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we calculate the conditional probabilities of: topic|document and of word|topic.\n",
    "\n",
    "p(topic|document) = # times topic appears in doc/total number of words in a document\n",
    "p(word|topic) = # times word appears in a topic/total words in topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Define conditional probability functions:\n",
    "def p_topic_given_document(topic, d, alpha=0.1):  #p(t|D), using smoothing of 0.1 like in Naive Bayes\n",
    "    return ((document_topic_counts[d][topic]+alpha)/(document_lengths[d]+K*alpha)) #\n",
    "\n",
    "def p_word_given_topic(word, topic, beta=0.1):  #p(w|t), using smoothing of 0.1 like in Naive Bayes\n",
    "    return ((topic_word_counts[topic][word]+beta)/(topic_counts[topic]+W*beta)) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Use these probabilities to create the weights to update topics:\n",
    "\n",
    "def topic_weight(d, word, k):\n",
    "    #return weight for the k-topic given doc and word\n",
    "    return p_word_given_topic(word, k)*p_topic_given_document(k, d)\n",
    "\n",
    "def choose_new_topic(d, word):\n",
    "    return sample_from([topic_weight(d, word, k) for k in range(K)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Start by randomly assigning every word to a random topic:\n",
    "random.seed(0)\n",
    "document_topics = [[random.randrange(K) for word in document] for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3, 3, 1, 1, 2, 1, 3],\n",
       " [1, 1, 2, 3, 2],\n",
       " [1, 3, 2, 1, 3, 3],\n",
       " [3, 3, 1, 2, 3],\n",
       " [2, 1, 0, 1],\n",
       " [2, 3, 3, 1, 3, 1],\n",
       " [3, 2, 0, 2],\n",
       " [1, 3, 2, 0],\n",
       " [1, 3, 0, 1],\n",
       " [3, 0, 2, 0],\n",
       " [3, 3, 1],\n",
       " [0, 1, 2, 3],\n",
       " [0, 2, 2],\n",
       " [2, 3, 2, 3, 2],\n",
       " [2, 1, 2]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_topics #How the topics were assigned, from 0 to 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Populate counters according with the randomly assigned topic per word:\n",
    "for d in range(D): #For each doc...\n",
    "    for word, topic in zip(documents[d], document_topics[d]): #For each word, topic in tuple (word, topic), populate...\n",
    "        document_topic_counts[d][topic] += 1  #document d, topic 'topic'\n",
    "        topic_word_counts[topic][word] += 1   #topic 'topic', word 'word'\n",
    "        topic_counts[topic] += 1          #add topic 'topic' count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 10, 0: 5, 2: 5})\n",
      "Counter({'Big Data': 5, 'decision trees': 3, 'Java': 3, 'neural networks': 3, 'C++': 3, 'mathematics': 3, 'pandas': 3})\n",
      "[40, 90, 95, 110]\n"
     ]
    }
   ],
   "source": [
    "#Sample of counters\n",
    "print document_topic_counts[4]\n",
    "print topic_word_counts[0]\n",
    "print topic_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, use the preliminary topic allocations and Gibbs sampling to determine the final topic allocations.\n",
    "\n",
    "Now, this is the neat part. We have a preliminary combo of (word, topic) per document. This allocation was done randomly with an uniform distribution from 0 to K. Using Gibbs sampling and the conditional p(t|d) and p(w|t), we can iteratively recalculate the (word,topic) combos such that the final values are consistent with the conditional probabilities as defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ite in range(10000): \n",
    "    for d in range(D): #For all documents\n",
    "        for i, (word, topic) in enumerate(zip(documents[d], document_topics[d])): #Iterate over (word, topic) for each document\n",
    "            \n",
    "            document_topic_counts[d][topic] -= 1  #substract one from current document/topic\n",
    "            topic_word_counts[topic][word] -= 1 #substract one from current topic/word\n",
    "            topic_counts[topic] -= 1 \n",
    "            document_lengths[d] -= 1\n",
    "            \n",
    "            #Randomly choose new topic according to new weights: Weight kth = p_word_given_topic(word, k)*p_topic_given_document(k, d)\n",
    "            #this is the \"magic\" part. We assume that the probability of a given word belonging to a topic equals the prior probability\n",
    "            #of p(w|t) times p(t|d)\n",
    "            new_topic = choose_new_topic(d, word)\n",
    "            document_topics[d][i] = new_topic #reasign new topic to ith word of dth document\n",
    "            \n",
    "            document_topic_counts[d][new_topic] += 1 #increase topic count again\n",
    "            topic_word_counts[new_topic][word] += 1\n",
    "            topic_counts[new_topic] += 1\n",
    "            document_lengths[d] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see now the most common words per topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = 0 , word = Big Data, counts = 5\n",
      "topic = 0 , word = neural networks, counts = 4\n",
      "topic = 0 , word = mathematics, counts = 3\n",
      "topic = 0 , word = pandas, counts = 3\n",
      "topic = 0 , word = C++, counts = 3\n",
      "topic = 1 , word = regression, counts = 6\n",
      "topic = 1 , word = HBase, counts = 4\n",
      "topic = 1 , word = statsmodels, counts = 4\n",
      "topic = 1 , word = NoSQL, counts = 3\n",
      "topic = 1 , word = Storm, counts = 3\n",
      "topic = 2 , word = Postgres, counts = 5\n",
      "topic = 2 , word = Python, counts = 5\n",
      "topic = 2 , word = probability, counts = 4\n",
      "topic = 2 , word = MongoDB, counts = 4\n",
      "topic = 2 , word = Cassandra, counts = 4\n",
      "topic = 3 , word = R, counts = 7\n",
      "topic = 3 , word = Java, counts = 5\n",
      "topic = 3 , word = statistics, counts = 5\n",
      "topic = 3 , word = scikit-learn, counts = 5\n",
      "topic = 3 , word = probability, counts = 4\n"
     ]
    }
   ],
   "source": [
    "for top_num, word_counts in enumerate(topic_word_counts):\n",
    "    for word, count in word_counts.most_common()[:5]:\n",
    "        print 'topic = ' + str(top_num) + ' , ' + 'word = ' + str(word) + ', counts = ' + str(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [insight_project]",
   "language": "python",
   "name": "Python [insight_project]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
